{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_all = {\n",
    "    \"GSM8K\": {\n",
    "        \"text-davinci-003\": pd.read_parquet(\"../data/gsm8k_100/faithfulness-scores-text-davinci-003.parquet\"),\n",
    "        \"gpt-3.5-turbo\": pd.read_parquet(\"../data/gsm8k_100/faithfulness-scores-gpt-3.5-turbo.parquet\"),\n",
    "    },\n",
    "    \"SVAMP\": {\n",
    "        \"text-davinci-003\": pd.read_parquet(\"../data/svamp_100/faithfulness-scores-text-davinci-003.parquet\"),\n",
    "        \"gpt-3.5-turbo\": pd.read_parquet(\"../data/svamp_100/faithfulness-scores-gpt-3.5-turbo.parquet\"),\n",
    "    },\n",
    "    \"ASDiv\": {\n",
    "        \"text-davinci-003\": pd.read_parquet(\"../data/asdiv_100/faithfulness-scores-text-davinci-003.parquet\"),\n",
    "        \"gpt-3.5-turbo\": pd.read_parquet(\"../data/asdiv_100/faithfulness-scores-gpt-3.5-turbo.parquet\"),\n",
    "    },\n",
    "    \"StrategyQA\": {\n",
    "        \"text-davinci-003\": pd.read_parquet(\"../data/strategyqa_100/faithfulness-scores-text-davinci-003.parquet\"),\n",
    "        \"gpt-3.5-turbo\": pd.read_parquet(\"../data/strategyqa_100/faithfulness-scores-gpt-3.5-turbo.parquet\"),\n",
    "    },\n",
    "    \"Sports Understanding\": {\n",
    "        \"text-davinci-003\": pd.read_parquet(\"../data/sportsunderstanding_100/faithfulness-scores-text-davinci-003.parquet\"),\n",
    "        \"gpt-3.5-turbo\": pd.read_parquet(\"../data/sportsunderstanding_100/faithfulness-scores-gpt-3.5-turbo.parquet\"),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "data = data_all[\"GSM8K\"][\"text-davinci-003\"]\n",
    "data = copy.deepcopy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama2-7B parameter model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_llama2_response(client, question):\n",
    "    time.sleep(1)\n",
    "    return client.text_generation(\n",
    "        question,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.01\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540045fc1b4e4b0f9f69d80c4f46d8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3096116264cf4babbd0d1692d90c671c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "YOUR_TOKEN = \"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\"\n",
    "client = InferenceClient(model=\"meta-llama/Llama-2-7b-chat-hf\", token=YOUR_TOKEN)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "data['experiment_1'] = [get_llama2_response(client, question) for question in tqdm(data['experiment_1_question'].to_list())]\n",
    "\n",
    "data['experiment_4'] = [\n",
    "    [get_llama2_response(client, question) for question in paraphrased_questions] \n",
    "    for paraphrased_questions in tqdm(data['experiment_4_question'].to_list())\n",
    "]\n",
    "\n",
    "data.to_parquet(\"gsm8k_llama2_7b.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf45756e83824ee3a7994d559fcae11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c334feb159644331af26e18aefd7c72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "YOUR_TOKEN = \"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\"\n",
    "client = InferenceClient(model=\"meta-llama/Llama-2-13b-chat-hf\", token=YOUR_TOKEN)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "data['experiment_1'] = [get_llama2_response(client, question) for question in tqdm(data['experiment_1_question'].to_list())]\n",
    "\n",
    "data['experiment_4'] = [\n",
    "    [get_llama2_response(client, question) for question in paraphrased_questions] \n",
    "    for paraphrased_questions in tqdm(data['experiment_4_question'].to_list())\n",
    "]\n",
    "\n",
    "data.to_parquet(\"gsm8k_llama2_13b.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576b4559ee3346a3b02f00d51fd6fb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9eece921cf4506b20a2d48ae06e80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "YOUR_TOKEN = \"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\"\n",
    "client = InferenceClient(model=\"meta-llama/Llama-2-70b-chat-hf\", token=YOUR_TOKEN)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "data['experiment_1'] = [get_llama2_response(client, question) for question in tqdm(data['experiment_1_question'].to_list())]\n",
    "\n",
    "data['experiment_4'] = [\n",
    "    [get_llama2_response(client, question) for question in paraphrased_questions] \n",
    "    for paraphrased_questions in tqdm(data['experiment_4_question'].to_list())\n",
    "]\n",
    "\n",
    "data.to_parquet(\"gsm8k_llama2_70b.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet(\"gam8k_llama2_13b.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet(\"gam8k_llama2_7b.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_parquet(\"gam8k_llama2_7b.parquet\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
