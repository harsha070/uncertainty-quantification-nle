{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6b2708-81b2-4615-82aa-779fb3f3de24",
   "metadata": {},
   "source": [
    "# Dataset Preparation for GSM8K, ASDiv, SVAMP, StrategyQA, Sports Understanding datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd5a2f8-5abb-4ca6-b56f-0eae032ea8e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OpenAI Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd48d57-b219-4c75-8529-8cab74eb9379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'use-your-api-token-here'\n",
    "import openai\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import ast\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "\n",
    "# Define model name. The following model is used to get paraphrased questions for a question\n",
    "MODEL_NAME = \"text-davinci-003\"\n",
    "\n",
    "def get_openai_response(task, question):\n",
    "    \"\"\"\n",
    "    task is a string prepended to a question to get natural language explanation.\n",
    "    \"\"\"\n",
    "    response = openai.Completion.create(\n",
    "        model=MODEL_NAME,\n",
    "        prompt=task + '\\n' + question,\n",
    "        temperature=0,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    return response.choices[0].text        \n",
    "        \n",
    "def get_openai_response_wrapper(args):\n",
    "    \"\"\"\n",
    "    wrapper around `get_openai_response` function for multithreading api calls\n",
    "    \"\"\"\n",
    "    task, question = args\n",
    "    return get_openai_response(task=task, question=question)\n",
    "\n",
    "\n",
    "def get_openai_response_batch(task, questions):\n",
    "    \"\"\"\n",
    "    Make concurrent calls to open ai api's for faster processing\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        args_list = [(task, question) for question in questions]\n",
    "        for result in tqdm(executor.map(get_openai_response_wrapper, args_list), total=len(questions)):\n",
    "            responses.append(result)\n",
    "    return responses\n",
    "\n",
    "\n",
    "def get_paraphrased_questions(questions):\n",
    "    \"\"\"\n",
    "    Generate semantically equivalent paraphrased questions for a question for sample probing uncertainty.\n",
    "    \"\"\"\n",
    "    responses = [\n",
    "        get_openai_response(\n",
    "            \"Paraphrase the question into 25 different forms with the same meaning, and share them as a Python list of double quotes enclosed strings\",\n",
    "            question) for question in tqdm(questions)]\n",
    "    responses = [ast.literal_eval(response) for response in tqdm(responses)]\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0299894a-8a44-4dd0-ba80-66408fc42a44",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GSM8K 100 Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25527f-1c3b-496c-9d6c-3e47e56f68f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load GSM8K dataset\n",
    "gsm8k_dataset = pd.DataFrame(load_dataset(\"gsm8k\", \"main\")[\"test\"])\n",
    "gsm8k_dataset[\"label\"] = gsm8k_dataset[\"answer\"].apply(lambda answer: answer[answer.find(\"####\") + 4:].strip())\n",
    "# Sample 100 questions at random\n",
    "gsm8k_dataset_subset = gsm8k_dataset.sample(n=100, random_state=42)\n",
    "data = gsm8k_dataset_subset\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6427cac3-9325-4623-80bf-da3435a30653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_question(question):\n",
    "    \"\"\"\n",
    "    Extract sentence from question from the full question. The paraphrases of extracted sentence are used to calculate uncertainty in sample probing experiment. \n",
    "    \"\"\"\n",
    "    for question_word in [\"how\", \"calculate\", \"what\"]:\n",
    "        match = re.search(r'\\b' + re.escape(question_word) + r'\\b', question, re.IGNORECASE)\n",
    "        if match:\n",
    "            question = question[match.start():]\n",
    "    return question\n",
    "\n",
    "# Extract questions from dataset\n",
    "questions = data[\"question\"].to_list()\n",
    "questions = list(map(parse_question, questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d1c879-e0e8-45b0-9259-4640f77b34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paraphrase questions for sample probing\n",
    "responses = get_openai_response_batch(\n",
    "    \"Paraphrase the question into 10 different forms with the same meaning, and share them as a Python list of double quotes enclosed strings\",\n",
    "    questions,\n",
    ")\n",
    "responses = [ast.literal_eval(response) for response in tqdm(responses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5873cfb-1cc5-43a9-916f-a696e24fb48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrased_questions = responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e296a-78a2-45d8-b125-1bad06b3fff4",
   "metadata": {},
   "source": [
    "## Experiment 1 - Verbalized Chain Of Thought\n",
    "## Experiment 2 - Feature Importance Explanation\n",
    "## Experiment 3 - Verbalized Feature Importance\n",
    "## Experiment 4 - Sample Probing CoT Explanation\n",
    "## Experiment 5 - Sample Probing Feature Importance Explanation\n",
    "## Experiment 8 - Model Probing CoT Explanation\n",
    "## Experiment 9 - Model Probing Feature Importance Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5362f9f4-7aaf-4c7d-be90-e9aa462e9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Q_e for different tasks. Q_e is preprended to question Q to generate answer A and natural language explanation A_e\n",
    "\n",
    "experiment_1_task = \"\"\"\n",
    "Read the question, give your answer by analyzing step by step, and assign a confidence level to each step and the final answer. The output format is as follows:\n",
    "Step 1: [Your reasoning here], Confidence: [Your confidence here]%\n",
    "Step 2: [Your reasoning here], Confidence: [Your confidence here]%\n",
    "Step 3: ...\n",
    "...\n",
    "Step N: [Your reasoning here], Confidence: [Your confidence here]%\n",
    "Final Answer and Overall Confidence (0-100): [Your answer as a number here], [Your confidence here]%\n",
    "Note: The confidence indicates the degree of certainty you have about your reasoning. For instance, if your confidence level is 80%, it means you are 80% certain that your reasoning is correct.\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_2_task = \"\"\"Read the question, and output the words important for your final answer, sorted in descending order of importance. The output format is as follows\n",
    "1. [Word 1 here]\n",
    "2. [Word 2 here]\n",
    "3. [Word 3 here]\n",
    "...\n",
    "N.: [Word N here]\n",
    "Final Answer and Overall Confidence (0-100): [Your answer as a number here], [Your confidence here]%\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_3_task = \"\"\"\n",
    "Read the question, and assign each word an importance score between 0 and 100 of how important it is for your answer. The output format is as follows\n",
    "Word: [Word 1 here], Importance: [Your importance score here]\n",
    "Word: [Word 2 here], Importance: [Your importance score here]\n",
    "Word: [Word 3 here], Importance: [Your importance score here]\n",
    "...\n",
    "Word: [Word N here], Importance: [Your importance score here]\n",
    "Final Answer and Overall Confidence (0-100): [Your answer as a number here], [Your confidence here]%\n",
    "Note: The importance scores of all words should add up to 100. The overall confidence score indicates the degree of certainty you have about your important words and importance scores. For instance, if your confidence level is 80%, it means you are 80% certain that important words and importance scores assigned are correct.\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_4_task = experiment_1_task\n",
    "\n",
    "experiment_5_task = experiment_2_task\n",
    "\n",
    "experiment_8_task = experiment_1_task\n",
    "\n",
    "experiment_9_task = experiment_2_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b1088-ac41-4ae6-b860-c21d718ce4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make OpenAI API calls to generate answers and natural language explanations\n",
    "\n",
    "data[\"experiment_1_question\"] = [experiment_1_task + \"\\n\" + question for question in data[\"question\"]]\n",
    "data[\"experiment_2_question\"] = [experiment_2_task + \"\\n\" + question for question in data[\"question\"]]\n",
    "data[\"experiment_3_question\"] = [experiment_3_task + \"\\n\" + question for question in data[\"question\"]]\n",
    "\n",
    "experiment_4_questions = []\n",
    "experiment_5_questions = []\n",
    "\n",
    "for idx, (_, row) in tqdm(enumerate(data.iterrows())):\n",
    "    question = row[\"question\"]\n",
    "    original_question = parse_question(question)\n",
    "    experiment_4_questions.append(\n",
    "        [experiment_4_task + \"\\n\" + question.replace(original_question, paraphrased_question) for paraphrased_question in paraphrased_questions[idx]]\n",
    "    )\n",
    "    experiment_5_questions.append(\n",
    "        [experiment_5_task + \"\\n\" + question.replace(original_question, paraphrased_question) for paraphrased_question in paraphrased_questions[idx]]\n",
    "    )\n",
    "    \n",
    "data[\"experiment_4_question\"] = experiment_4_questions\n",
    "data[\"experiment_5_question\"] = experiment_5_questions\n",
    "\n",
    "data[\"experiment_8_question\"] = [experiment_8_task + \"\\n\" + question for question in data[\"question\"]]\n",
    "data[\"experiment_9_question\"] = [experiment_9_task + \"\\n\" + question for question in data[\"question\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed6ee2-c28a-47aa-9e2c-6cabd4417e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet(\"data/gsm8k_100/input.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da331ea4-943e-44b1-aaa9-304e342b70bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AQUA 100 Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e69f6-030b-4f27-8c91-4ba4a915224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqua_dataset = pd.DataFrame(load_dataset(\"aqua_rat\")[\"test\"])\n",
    "aqua_dataset[\"inputs\"] = aqua_dataset[\"question\"] + \"\\n\" + aqua_dataset[\"options\"].apply(lambda x: \" \".join(x))\n",
    "aqua_dataset = aqua_dataset.sample(n=100, random_state=42)\n",
    "data = aqua_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27263ce-25df-46c8-a44b-30362b3c297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_question(question):\n",
    "    \"\"\"\n",
    "    Extract sentence from question from the full question. The paraphrases of extracted sentence are used to calculate uncertainty in sample probing experiment. \n",
    "    \"\"\"\n",
    "    for question_word in [\"how\", \"calculate\", \"what\", \"which\", \"find\", \"compute\"]:\n",
    "        match = re.search(r'\\b' + re.escape(question_word) + r'\\b', question, re.IGNORECASE)\n",
    "        if match:\n",
    "            question = question[match.start():]\n",
    "    return question\n",
    "\n",
    "questions = []\n",
    "for idx, question in enumerate(data[\"question\"]):\n",
    "    questions.append(parse_question(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76d6a3-a9de-49d6-b599-b361d1de3047",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3088a-64f6-4217-bf7c-8aed2f590c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = get_openai_response_batch(\n",
    "    \"Paraphrase the question into 10 different forms with the same meaning, and share them as a Python list of double quotes enclosed strings\",\n",
    "    questions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d92838-5ba0-4580-bfc4-68e5d22c23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(response):\n",
    "    try:\n",
    "        return ast.literal_eval(response)\n",
    "    except:\n",
    "        return [ques[ques.find(\"\\\"\") + 1: ques.rfind(\"\\\"\")] for ques in response.strip().split(\"\\n\")]\n",
    "    \n",
    "paraphrased_questions = list(map(parse_response, responses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f89de-6ae1-43b5-9362-f5ac12b131c2",
   "metadata": {},
   "source": [
    "## Experiment 1 - Verbalized Chain Of Thought\n",
    "## Experiment 2 - Feature Importance Explanation\n",
    "## Experiment 3 - Verbalized Feature Importance\n",
    "## Experiment 4 - Sample Probing CoT Explanation\n",
    "## Experiment 5 - Sample Probing Feature Importance Explanation\n",
    "## Experiment 8 - Model Probing CoT Explanation\n",
    "## Experiment 9 - Model Probing Feature Importance Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd7f234-2dc3-47de-adf2-b0f3ede91469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Q_e for different tasks. Q_e is preprended to question Q to generate answer A and natural language explanation A_e\n",
    "\n",
    "experiment_1_task = \"\"\"Read the question, give your answer by analyzing step by step, and assign a confidence level to each step and the final answer. The output format is as follows:\n",
    "Step 1: [Your reasoning here], Confidence: [Your confidence here]%\n",
    "Step 2: [Your reasoning here], Confidence: [Your confidence here]%\n",
    "Step 3: ...\n",
    "...\n",
    "Step N: [Your reasoning here], Confidence: [Your confidence here]%\n",
    "Final Answer and Overall Confidence (0-100): [Your answer - Option A / B / C / D / E], [Your confidence here]%\n",
    "Note: The confidence indicates the degree of certainty you have about your answer. For instance, if your confidence level is 80%, it means you are 80% certain that your answer is correct.\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_2_task = \"\"\"Read the question, and output the words important for your final answer, sorted in descending order of importance. The output format is as follows\n",
    "1. [Word 1 here]\n",
    "2. [Word 2 here]\n",
    "3. [Word 3 here]\n",
    "...\n",
    "N.: [Word N here]\n",
    "Final Answer and Overall Confidence (0-100): [Your answer - Option A / B / C / D / E], [Your confidence here]%\n",
    "Provide the answer in aforementioned format, and nothing else.\"\"\"\n",
    "\n",
    "experiment_3_task = \"\"\"Read the question, and assign each word an importance score between 0 and 100 of how important it is for your answer. The output format is as follows\n",
    "Word: [Word 1 here], Importance: [Your importance score here]\n",
    "Word: [Word 2 here], Importance: [Your importance score here]\n",
    "Word: [Word 3 here], Importance: [Your importance score here]\n",
    "...\n",
    "Word: [Word N here], Importance: [Your importance score here]\n",
    "Final Answer and Overall Confidence (0-100): [Your answer - Option A / B / C / D / E], [Your confidence here]%\n",
    "Note: The importance scores of all words should add up to 100. The overall confidence score indicates the degree of certainty you have about your important words and importance scores. For instance, if your confidence level is 80%, it means you are 80% certain that important words and importance scores assigned are correct.\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_4_task = experiment_1_task\n",
    "\n",
    "experiment_5_task = experiment_2_task\n",
    "\n",
    "experiment_8_task = experiment_1_task\n",
    "\n",
    "experiment_9_task = experiment_2_task\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb1a2a-e5f4-40fc-a3ca-a8d8c390620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"experiment_1_question\"] = [experiment_1_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "data[\"experiment_2_question\"] = [experiment_2_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "data[\"experiment_3_question\"] = [experiment_3_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "\n",
    "experiment_4_questions = []\n",
    "experiment_5_questions = []\n",
    "\n",
    "for idx, (_, row) in tqdm(enumerate(data.iterrows())):\n",
    "    question = row[\"inputs\"]\n",
    "    original_question = questions[idx]\n",
    "    experiment_4_questions.append(\n",
    "        [experiment_4_task + \"\\n\" + question.replace(original_question, paraphrased_question) for paraphrased_question in set(paraphrased_questions[idx])]\n",
    "    )\n",
    "    experiment_5_questions.append(\n",
    "        [experiment_5_task + \"\\n\" + question.replace(original_question, paraphrased_question) for paraphrased_question in set(paraphrased_questions[idx])]\n",
    "    )\n",
    "    \n",
    "data[\"experiment_4_question\"] = experiment_4_questions\n",
    "data[\"experiment_5_question\"] = experiment_5_questions\n",
    "\n",
    "data[\"experiment_8_question\"] = [experiment_8_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "data[\"experiment_9_question\"] = [experiment_9_task + \"\\n\" + question for question in data[\"inputs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46bc95a-e146-484c-afbc-bcfe0772e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_parquet(\"data/aqua_100/input.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4704d3e-cb67-48f5-bd86-3c63698595a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.iloc[0][\"experiment_4_question\"][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c696d0-9e8d-4171-9f91-bb9061fca99b",
   "metadata": {},
   "source": [
    "# ASDiv 100 Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65734cf-a36e-45ed-bd39-dbaecd960d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "asdiv_dataset = pd.DataFrame(load_dataset(\"EleutherAI/asdiv\")[\"validation\"])\n",
    "asdiv_dataset = asdiv_dataset.sample(n=100, random_state=42)\n",
    "asdiv_dataset[\"inputs\"] = asdiv_dataset[\"body\"] + asdiv_dataset[\"question\"]\n",
    "data = asdiv_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869f89e4-eef9-4901-96af-e000d8836292",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf8eef-a491-48fb-9fbf-2ea01c354063",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data[\"question\"].to_list()\n",
    "\n",
    "responses = get_openai_response_batch(\n",
    "    \"Paraphrase the question into 10 different forms with the exact same meaning, and share them as a Python list of double quotes enclosed strings\",\n",
    "    questions,\n",
    ")\n",
    "\n",
    "def parse_response(response):\n",
    "    try:\n",
    "        return ast.literal_eval(response)\n",
    "    except:\n",
    "        return [ques[ques.find(\"\\\"\") + 1: ques.rfind(\"\\\"\")] for ques in response.strip().split(\"\\n\")]\n",
    "    \n",
    "paraphrased_questions = list(map(parse_response, responses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42719c-81ee-4276-b447-5f6cb5fb0419",
   "metadata": {},
   "source": [
    "## Experiment 1 - Verbalized Chain Of Thought\n",
    "## Experiment 2 - Feature Importance Explanation\n",
    "## Experiment 3 - Verbalized Feature Importance\n",
    "## Experiment 4 - Sample Probing CoT Explanation\n",
    "## Experiment 5 - Sample Probing Feature Importance Explanation\n",
    "## Experiment 8 - Model Probing CoT Explanation\n",
    "## Experiment 9 - Model Probing Feature Importance Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df92a108-ff62-4368-aaae-8fa2d78e83e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Q_e for different tasks. Q_e is preprended to question Q to generate answer A and natural language explanation A_e\n",
    "\n",
    "experiment_1_task = \"\"\"\n",
    "Read the question, give your answer by analyzing step by step, and assign a confidence level to each step and the final answer. The output format is as follows:\n",
    "Step 1: [Your reasoning here], Confidence: [Your confidence here]%\n",
    "Step 2: ...\n",
    "Step 3: ...\n",
    "...\n",
    "Step N: ...\n",
    "Final Answer and Overall Confidence (0-100): [Your answer as a number here], [Your confidence here]%\n",
    "Note: The confidence indicates the degree of certainty you have about your answer. For instance, if your confidence level is 80%, it means you are 80% certain that your answer is correct.\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_2_task = \"\"\"Read the question, and output the words important for your final answer, sorted in descending order of importance. The output format is as follows\n",
    "1. [Word 1 here]\n",
    "2. [Word 2 here]\n",
    "3. [Word 3 here]\n",
    "...\n",
    "N.: [Word N here]\n",
    "Final Answer and Overall Confidence (0-100): [Your answer as a number here], [Your confidence here]%\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_3_task = \"\"\"\n",
    "Read the question, and assign each word an importance score between 0 and 100 of how important it is for your final answer. The output format is as follows\n",
    "Word: [Word 1 here], Importance: [Your importance score here]\n",
    "Word: [Word 2 here], Importance: [Your importance score here]\n",
    "Word: [Word 3 here], Importance: [Your importance score here]\n",
    "...\n",
    "Word: [Word N here], Importance: [Your importance score here]\n",
    "Final Answer and Overall Confidence (0-100): [Your answer as a number here], [Your confidence here]%\n",
    "Note: The importance scores of all words should add up to 100. The overall confidence score indicates the degree of certainty you have about your importance scores. For instance, if your confidence level is 80%, it means you are 80% certain that importance scores assigned are correct.\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_4_task = experiment_1_task\n",
    "\n",
    "experiment_5_task = experiment_2_task\n",
    "\n",
    "experiment_8_task = experiment_1_task\n",
    "\n",
    "experiment_9_task = experiment_2_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a24984f-8711-4a38-91d8-7d86f256b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make OpenAI API calls to generate answers and natural language explanations\n",
    "\n",
    "data[\"experiment_1_question\"] = [experiment_1_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "data[\"experiment_2_question\"] = [experiment_2_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "data[\"experiment_3_question\"] = [experiment_3_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "\n",
    "experiment_4_questions = []\n",
    "experiment_5_questions = []\n",
    "\n",
    "for idx, (_, row) in tqdm(enumerate(data.iterrows())):\n",
    "    question = row[\"inputs\"]\n",
    "    original_question = row[\"question\"]\n",
    "    experiment_4_questions.append(\n",
    "        [experiment_4_task + \"\\n\" + question.replace(original_question, paraphrased_question) for paraphrased_question in set(paraphrased_questions[idx])]\n",
    "    )\n",
    "    experiment_5_questions.append(\n",
    "        [experiment_5_task + \"\\n\" + question.replace(original_question, paraphrased_question) for paraphrased_question in set(paraphrased_questions[idx])]\n",
    "    )\n",
    "    \n",
    "data[\"experiment_4_question\"] = experiment_4_questions\n",
    "data[\"experiment_5_question\"] = experiment_5_questions\n",
    "\n",
    "data[\"experiment_8_question\"] = [experiment_8_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "data[\"experiment_9_question\"] = [experiment_9_task + \"\\n\" + question for question in data[\"inputs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b113b-499b-4e06-b071-5875b428c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/asdiv_100/input.parquet\"\n",
    "# assert os.path.exists(file_path) is False\n",
    "data.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea785c2-cb40-4f5f-864d-b98beaa732b7",
   "metadata": {},
   "source": [
    "# SVAMP 100 Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75cb6ad-9a88-4e15-88c6-bd7ebae84ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "svamp_dataset = pd.DataFrame(load_dataset(\"ChilleD/SVAMP\")[\"test\"])\n",
    "svamp_dataset = svamp_dataset.sample(n=100, random_state=42)\n",
    "svamp_dataset[\"inputs\"] = svamp_dataset[\"Body\"] + \"\\n\" + svamp_dataset[\"Question\"]\n",
    "data = svamp_dataset\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338bfd18-066d-4d88-a00b-526347dea4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data[\"Question\"].to_list()\n",
    "\n",
    "responses = get_openai_response_batch(\n",
    "    \"Paraphrase the question into 10 different forms with the same meaning, and share them as a Python list of double quotes enclosed strings\",\n",
    "    questions,\n",
    ")\n",
    "\n",
    "def parse_response(response):\n",
    "    try:\n",
    "        return ast.literal_eval(response)\n",
    "    except:\n",
    "        return [ques[ques.find(\"\\\"\") + 1: ques.rfind(\"\\\"\")] for ques in response.strip().split(\"\\n\")]\n",
    "    \n",
    "paraphrased_questions = list(map(parse_response, responses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fbd914-3a97-48ce-9db8-21b509103da4",
   "metadata": {},
   "source": [
    "## Experiment 1 - Verbalized Chain Of Thought\n",
    "## Experiment 2 - Feature Importance Explanation\n",
    "## Experiment 3 - Verbalized Feature Importance\n",
    "## Experiment 4 - Sample Probing CoT Explanation\n",
    "## Experiment 5 - Sample Probing Feature Importance Explanation\n",
    "## Experiment 8 - Model Probing CoT Explanation\n",
    "## Experiment 9 - Model Probing Feature Importance Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1a694-e2fe-429d-a2fd-a8e80c7fcddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Q_e for different tasks. Q_e is preprended to question Q to generate answer A and natural language explanation A_e\n",
    "\n",
    "experiment_1_task = \"\"\"\n",
    "Read the question, give your answer by analyzing step by step, and assign a confidence level to each step and the final answer. The output format is as follows:\n",
    "Step 1: [Your reasoning here], Confidence: [Your confidence here]%\n",
    "Step 2: ...\n",
    "Step 3: ...\n",
    "...\n",
    "Step N: ...\n",
    "Final Answer and Overall Confidence (0-100): [Your answer as a number here], [Your confidence here]%\n",
    "Note: The confidence indicates the degree of certainty you have about your answer. For instance, if your confidence level is 80%, it means you are 80% certain that your answer is correct.\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_2_task = \"\"\"Read the question, and output the words important for your final answer, sorted in descending order of importance. The output format is as follows\n",
    "1. [Word 1 here]\n",
    "2. [Word 2 here]\n",
    "3. [Word 3 here]\n",
    "...\n",
    "N.: [Word N here]\n",
    "Final Answer and Overall Confidence (0-100): [Your answer as a number here], [Your confidence here]%\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_3_task = \"\"\"\n",
    "Read the question, and assign each word an importance score between 0 and 100 of how important it is for your final answer. The output format is as follows\n",
    "Word: [Word 1 here], Importance: [Your importance score here]\n",
    "Word: [Word 2 here], Importance: [Your importance score here]\n",
    "Word: [Word 3 here], Importance: [Your importance score here]\n",
    "...\n",
    "Word: [Word N here], Importance: [Your importance score here]\n",
    "Final Answer and Overall Confidence (0-100): [Your answer as a number here], [Your confidence here]%\n",
    "Note: The importance scores of all words should add up to 100. The overall confidence score indicates the degree of certainty you have about your importance scores. For instance, if your confidence level is 80%, it means you are 80% certain that importance scores assigned are correct.\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_4_task = experiment_1_task\n",
    "\n",
    "experiment_5_task = experiment_2_task\n",
    "\n",
    "experiment_8_task = experiment_1_task\n",
    "\n",
    "experiment_9_task = experiment_2_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf762e-8558-400b-a1c3-d178f9d4aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make OpenAI API calls to generate answers and natural language explanations\n",
    "\n",
    "data[\"experiment_1_question\"] = [experiment_1_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "data[\"experiment_2_question\"] = [experiment_2_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "data[\"experiment_3_question\"] = [experiment_3_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "\n",
    "experiment_4_questions = []\n",
    "experiment_5_questions = []\n",
    "\n",
    "for idx, (_, row) in tqdm(enumerate(data.iterrows())):\n",
    "    question = row[\"inputs\"]\n",
    "    original_question = row[\"Question\"]\n",
    "    experiment_4_questions.append(\n",
    "        [experiment_4_task + \"\\n\" + question.replace(original_question, paraphrased_question) for paraphrased_question in set(paraphrased_questions[idx])]\n",
    "    )\n",
    "    experiment_5_questions.append(\n",
    "        [experiment_5_task + \"\\n\" + question.replace(original_question, paraphrased_question) for paraphrased_question in set(paraphrased_questions[idx])]\n",
    "    )\n",
    "    \n",
    "data[\"experiment_4_question\"] = experiment_4_questions\n",
    "data[\"experiment_5_question\"] = experiment_5_questions\n",
    "\n",
    "data[\"experiment_8_question\"] = [experiment_8_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "data[\"experiment_9_question\"] = [experiment_9_task + \"\\n\" + question for question in data[\"inputs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ce188-06e2-42bc-92b3-896f86231715",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.iloc[0][\"experiment_9_question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15801aa8-6b57-4000-b803-9986c6a4d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/svamp_100/input.parquet\"\n",
    "assert os.path.exists(file_path) is False\n",
    "data.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a75160-f940-4d48-9623-10b03a186e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_parquet(\"data/svamp_100/output-gpt-3.5-turbo.parquet\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f190d-9bbf-4cc4-9b27-c3343d91d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = \"\"\"Read the question, and identify words important for your final answer, sorted in descending order of importance. The output format is as follows\n",
    "Word: [Word 1 here]\n",
    "Word: [Word 2 here]\n",
    "Word: [Word 3 here]\n",
    "...\n",
    "Word: [Word N here]\n",
    "Final Answer: [Your answer as a number here]\n",
    "Provide the answer in aforementioned format, and nothing else.\"\"\"\n",
    "\n",
    "data[\"experiment_2_question\"] = [experiment_2_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "\n",
    "data[\"experiment_5_question\"] = data[\"experiment_5_question\"].apply(\n",
    "    lambda text_list: [text.replace(temp, experiment_2_task) for text in text_list]\n",
    ")\n",
    "\n",
    "data[\"experiment_9_question\"] = [experiment_9_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "\n",
    "data.to_parquet(\"data/svamp_100/output-gpt-3.5-turbo.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c94655-120e-4377-836c-15c43ddca030",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.iloc[89][\"experiment_5_question\"][7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab82476-d142-45be-8556-8ac696dd94d4",
   "metadata": {},
   "source": [
    "# Sports Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2fb2a4-8994-47c0-9dd0-347b43fe9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "sports_dataset = pd.DataFrame(load_dataset(\"tasksource/bigbench\",'sports_understanding')[\"validation\"])\n",
    "data = sports_dataset.sample(n=100, random_state=42)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a6cedf-4665-405c-b379-01bb88451ce2",
   "metadata": {},
   "source": [
    "## Experiment 1 - Verbalized Chain Of Thought\n",
    "## Experiment 2 - Feature Importance Explanation\n",
    "## Experiment 3 - Verbalized Feature Importance\n",
    "## Experiment 4 - Sample Probing CoT Explanation\n",
    "## Experiment 5 - Sample Probing Feature Importance Explanation\n",
    "## Experiment 8 - Model Probing CoT Explanation\n",
    "## Experiment 9 - Model Probing Feature Importance Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e7560a-dde6-4e35-9180-1a285d94919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Q_e for different tasks. Q_e is preprended to question Q to generate answer A and natural language explanation A_e\n",
    "\n",
    "experiment_1_task = \"\"\"\n",
    "Read the question, give your answer by analyzing step by step, and assign a confidence level to each step and the final answer. The output format is as follows:\n",
    "Step 1: [Your reasoning here], Confidence: [Your confidence here]%\n",
    "Step 2: ...\n",
    "Step 3: ...\n",
    "...\n",
    "Step N: ...\n",
    "Final Answer and Overall Confidence (0-100): [Your answer plausible / implausible here], [Your confidence here]%\n",
    "Note: The confidence indicates the degree of certainty you have about your answer. For instance, if your confidence level is 80%, it means you are 80% certain that your answer is correct.\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_2_task = \"\"\"Read the question, and output the words important for your final answer, sorted in descending order of importance. The output format is as follows\n",
    "1. [Word 1 here]\n",
    "2. [Word 2 here]\n",
    "3. [Word 3 here]\n",
    "...\n",
    "N.: [Word N here]\n",
    "Final Answer and Overall Confidence (0-100): [Your answer plausible / implausible here], [Your confidence here]%\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_3_task = \"\"\"\n",
    "Read the question, and assign each word an importance score between 0 and 100 of how important it is for your final answer. The output format is as follows\n",
    "Word: [Word 1 here], Importance: [Your importance score here]\n",
    "Word: [Word 2 here], Importance: [Your importance score here]\n",
    "Word: [Word 3 here], Importance: [Your importance score here]\n",
    "...\n",
    "Word: [Word N here], Importance: [Your importance score here]\n",
    "Final Answer and Overall Confidence (0-100): [Your answer plausible / implausible here], [Your confidence here]%\n",
    "Note: The importance scores of all words should add up to 100. The overall confidence score indicates the degree of certainty you have about your importance scores. For instance, if your confidence level is 80%, it means you are 80% certain that importance scores assigned are correct.\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_4_task = experiment_1_task\n",
    "\n",
    "experiment_5_task = experiment_2_task\n",
    "\n",
    "experiment_8_task = experiment_1_task\n",
    "\n",
    "experiment_9_task = experiment_2_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e556e540-9f5d-4696-ae08-d625adee36ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_question(ques):\n",
    "    \"\"\"\n",
    "    Extract sentence from question from the full question. The paraphrases of extracted sentence are used to calculate uncertainty in sample probing experiment. \n",
    "    \"\"\"\n",
    "    return ques.strip().split(\"\\n\")[1][11:]\n",
    "\n",
    "questions = data[\"inputs\"].apply(parse_question).to_list()\n",
    "\n",
    "responses = get_openai_response_batch(\n",
    "    \"Paraphrase the question into 10 different forms with the same meaning, and share them as a Python list of double quotes enclosed strings\",\n",
    "    questions,\n",
    ")\n",
    "\n",
    "def parse_response(response):\n",
    "    try:\n",
    "        return ast.literal_eval(response)\n",
    "    except:\n",
    "        return [ques[ques.find(\"\\\"\") + 1: ques.rfind(\"\\\"\")] for ques in response.strip().split(\"\\n\")]\n",
    "    \n",
    "paraphrased_questions = list(map(parse_response, responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f2cdc6-df3a-4fa5-ba85-fa0feeaf9c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"paraphrased_questions\"] = paraphrased_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1640fef8-f0b8-4cfa-84e1-85f2c24ec5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make OpenAI API calls to generate answers and natural language explanations\n",
    "\n",
    "data[\"experiment_1_question\"] = [experiment_1_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "data[\"experiment_2_question\"] = [experiment_2_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "data[\"experiment_3_question\"] = [experiment_3_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "\n",
    "experiment_4_questions = []\n",
    "experiment_5_questions = []\n",
    "\n",
    "for idx, (_, row) in tqdm(enumerate(data.iterrows())):\n",
    "    question = row[\"inputs\"]\n",
    "    original_question = parse_question(question)\n",
    "    experiment_4_questions.append(\n",
    "        [experiment_4_task + \"\\n\" + question.replace(original_question, paraphrased_question) for paraphrased_question in set(paraphrased_questions[idx])]\n",
    "    )\n",
    "    experiment_5_questions.append(\n",
    "        [experiment_5_task + \"\\n\" + question.replace(original_question, paraphrased_question) for paraphrased_question in set(paraphrased_questions[idx])]\n",
    "    )\n",
    "    \n",
    "data[\"experiment_4_question\"] = experiment_4_questions\n",
    "data[\"experiment_5_question\"] = experiment_5_questions\n",
    "\n",
    "data[\"experiment_8_question\"] = [experiment_8_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "data[\"experiment_9_question\"] = [experiment_9_task + \"\\n\" + question for question in data[\"inputs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a194024-9959-4796-97fe-1f463066a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/sportsunderstanding_100/input.parquet\"\n",
    "assert os.path.exists(file_path) is False\n",
    "data.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454ba278-61ab-43b9-9ca6-bec4b96e63eb",
   "metadata": {},
   "source": [
    "# Strategy QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376648d-7a9d-473f-a4bb-2aaa4bef4f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "strategyqa_data = pd.DataFrame(load_dataset(\"ChilleD/StrategyQA\")[\"test\"])\n",
    "data = strategyqa_data.sample(n=100, random_state=42)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2418b047-696a-48bb-989b-9a79e473c3ce",
   "metadata": {},
   "source": [
    "## Experiment 1 - Verbalized Chain Of Thought\n",
    "## Experiment 2 - Feature Importance Explanation\n",
    "## Experiment 3 - Verbalized Feature Importance\n",
    "## Experiment 4 - Sample Probing CoT Explanation\n",
    "## Experiment 5 - Sample Probing Feature Importance Explanation\n",
    "## Experiment 8 - Model Probing CoT Explanation\n",
    "## Experiment 9 - Model Probing Feature Importance Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ea198-019d-4f44-84bc-7ba556c68b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Q_e for different tasks. Q_e is preprended to question Q to generate answer A and natural language explanation A_e\n",
    "\n",
    "experiment_1_task = \"\"\"\n",
    "Read the question, give your answer by analyzing step by step, and assign a confidence level to each step and the final answer. The output format is as follows:\n",
    "Step 1: [Your reasoning here], Confidence: [Your confidence here]%\n",
    "Step 2: ...\n",
    "Step 3: ...\n",
    "...\n",
    "Step N: ...\n",
    "Final Answer and Overall Confidence (0-100): [Your answer Yes/No here], [Your confidence here]%\n",
    "Note: The confidence indicates the degree of certainty you have about your answer. For instance, if your confidence level is 80%, it means you are 80% certain that your reasoning is correct.\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_2_task = \"\"\"Read the question, and output the words important for your final answer, sorted in descending order of importance. The output format is as follows\n",
    "1. [Word 1 here]\n",
    "2. [Word 2 here]\n",
    "3. [Word 3 here]\n",
    "...\n",
    "N.: [Word N here]\n",
    "Final Answer and Overall Confidence (0-100): [Your answer Yes/No here], [Your confidence here]%\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_3_task = \"\"\"\n",
    "Read the question, and assign each word an importance score between 0 and 100 of how important it is for your final answer. The output format is as follows\n",
    "Word: [Word 1 here], Importance: [Your importance score here]\n",
    "Word: [Word 2 here], Importance: [Your importance score here]\n",
    "Word: [Word 3 here], Importance: [Your importance score here]\n",
    "...\n",
    "Word: [Word N here], Importance: [Your importance score here]\n",
    "Final Answer and Overall Confidence (0-100): [Your answer Yes/No here], [Your confidence here]%\n",
    "Note: The importance scores of all words should add up to 100. The overall confidence score indicates the degree of certainty you have about your importance scores. For instance, if your confidence level is 80%, it means you are 80% certain that importance scores assigned are correct.\n",
    "Provide the answer in aforementioned format, and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "experiment_4_task = experiment_1_task\n",
    "\n",
    "experiment_5_task = experiment_2_task\n",
    "\n",
    "experiment_8_task = experiment_1_task\n",
    "\n",
    "experiment_9_task = experiment_2_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c780f482-d20d-40dc-bbd1-f82eb8f4c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data[\"question\"].to_list()\n",
    "\n",
    "responses = get_openai_response_batch(\n",
    "    \"Paraphrase the question into 10 different forms with the same meaning, and share them as a Python list of double quotes enclosed strings\",\n",
    "    questions,\n",
    ")\n",
    "\n",
    "def parse_response(response):\n",
    "    try:\n",
    "        return ast.literal_eval(response)\n",
    "    except:\n",
    "        return [ques[ques.find(\"\\\"\") + 1: ques.rfind(\"\\\"\")] for ques in response.strip().split(\"\\n\")]\n",
    "    \n",
    "paraphrased_questions = list(map(parse_response, responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde7a7ae-8b39-4ffb-be36-9ce39b3c34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"paraphrased_questions\"] = paraphrased_questions\n",
    "data[\"inputs\"] = data[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e16794c-0e09-4a85-b473-2c75e736955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make OpenAI API calls to generate answers and natural language explanations\n",
    "\n",
    "data[\"experiment_1_question\"] = [experiment_1_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "data[\"experiment_2_question\"] = [experiment_2_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "data[\"experiment_3_question\"] = [experiment_3_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "\n",
    "experiment_4_questions = []\n",
    "experiment_5_questions = []\n",
    "\n",
    "for idx, (_, row) in tqdm(enumerate(data.iterrows())):\n",
    "    question = row[\"question\"]\n",
    "    original_question = row[\"question\"]\n",
    "    experiment_4_questions.append(\n",
    "        [experiment_4_task + \"\\n\" + question.replace(original_question, paraphrased_question) for paraphrased_question in set(paraphrased_questions[idx])]\n",
    "    )\n",
    "    experiment_5_questions.append(\n",
    "        [experiment_5_task + \"\\n\" + question.replace(original_question, paraphrased_question) for paraphrased_question in set(paraphrased_questions[idx])]\n",
    "    )\n",
    "    \n",
    "data[\"experiment_4_question\"] = experiment_4_questions\n",
    "data[\"experiment_5_question\"] = experiment_5_questions\n",
    "\n",
    "data[\"experiment_8_question\"] = [experiment_8_task + \"\\n\" + question for question in data[\"inputs\"]]\n",
    "data[\"experiment_9_question\"] = [experiment_9_task + \"\\n\" + question for question in data[\"inputs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156e255f-e881-4dfe-8fed-f9fd881c2b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/strategyqa_100/input.parquet\"\n",
    "assert os.path.exists(file_path) is False\n",
    "data.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b8ec75-c805-4ee8-9e0c-4100a5a273be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a0c0c1-de73-4c5a-9f77-bf25cc2430f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
